# 04_ai_safety_and_guardrails.ipynb

import vertexai
from vertexai.generative_models import GenerativeModel, SafetySetting

def generate_safe_content(prompt):
    """
    Demonstrates implementation of Safety Settings to block harmful content.
    Crucial for Enterprise AI Governance.
    """
    model = GenerativeModel("gemini-1.0-pro")
    
    # Configure strict safety filters
    safety_settings = [
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
        ),
    ]
    
    try:
        response = model.generate_content(
            prompt,
            safety_settings=safety_settings
        )
        return response.text
    except Exception as e:
        return f"Content blocked due to safety filters: {e}"

# Example Usage
# print(generate_safe_content("Write a hate speech regarding...")) # Should trigger block
